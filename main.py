import http.server
import socketserver
import json
import os
import threading
import time
import requests
import yt_dlp
from datetime import datetime, timedelta
import io

# --- ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ú‡¶ø‡¶ï ‡¶≤‡¶æ‡¶á‡¶¨‡ßç‡¶∞‡ßá‡¶∞‡¶ø (Pillow) ---
try:
    from PIL import Image, ImageDraw, ImageFont, ImageFilter
    PILLOW_AVAILABLE = True
except ImportError:
    PILLOW_AVAILABLE = False
    print("‚ö†Ô∏è WARNING: Pillow library not found!")

# --- ‡¶ï‡¶®‡¶´‡¶ø‡¶ó‡¶æ‡¶∞‡ßá‡¶∂‡¶® ---
PORT = 8080
CONFIG_FILE = "config.json"
DB_FILE = "news_db.json"
NEWS_API_KEY = "pub_102fa773efa04ad2871534886e425eab"
RETENTION_HOURS = 48
PROMO_IMAGE_FILE = "promo_image.jpg"

# ‡¶´‡¶®‡ßç‡¶ü ‡¶ï‡¶®‡¶´‡¶ø‡¶ó‡¶æ‡¶∞‡ßá‡¶∂‡¶® (‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡¶æ ‡¶´‡¶æ‡¶á‡¶≤‡ßá‡¶∞ ‡¶®‡¶æ‡¶Æ‡ßá‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶Æ‡¶ø‡¶≤ ‡¶•‡¶æ‡¶ï‡¶§‡ßá ‡¶π‡¶¨‡ßá)
FONTS = {
    'bn': 'bn.ttf',  # ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø
    'hi': 'hi.ttf',  # ‡¶π‡¶ø‡¶®‡ßç‡¶¶‡¶ø‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø
    'en': 'en.ttf',  # ‡¶á‡¶Ç‡¶≤‡¶ø‡¶∂ ‡¶¨‡¶æ ‡¶°‡¶ø‡¶´‡¶≤‡ßç‡¶ü
    'tm': 'en.ttf'   # ‡¶§‡¶æ‡¶Æ‡¶ø‡¶≤‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø 
}

# ==========================================
# üß† PART 1: THE ROBOT BRAIN (News Hunter)
# ==========================================

def load_config():
    if not os.path.exists(CONFIG_FILE): return {}
    with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
        return json.load(f)

def load_db():
    if not os.path.exists(DB_FILE): return []
    try:
        with open(DB_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data.get("news", [])
    except: return []

def get_smart_date():
    today = datetime.now()
    yesterday = today - timedelta(days=1)
    return today.strftime("%Y%m%d"), yesterday.strftime("%Y%m%d")

def clean_old_news(news_list):
    current_time = time.time()
    retention_seconds = RETENTION_HOURS * 3600
    return [n for n in news_list if (current_time - n.get('timestamp', 0)) < retention_seconds]

# --- ‡¶á‡¶â‡¶®‡¶ø‡¶≠‡¶æ‡¶∞‡ßç‡¶∏‡¶æ‡¶≤ ‡¶≠‡¶ø‡¶°‡¶ø‡¶ì ‡¶°‡¶ø‡¶ü‡ßá‡¶ï‡ßç‡¶ü‡¶∞ (YouTube + Facebook + Insta) ---
def get_embed_code(url, video_id):
    # ‡¶™‡ßç‡¶≤‡ßç‡¶Ø‡¶æ‡¶ü‡¶´‡¶∞‡ßç‡¶Æ ‡¶ö‡¶ø‡¶®‡ßá ‡¶∏‡¶†‡¶ø‡¶ï ‡¶è‡¶Æ‡ßç‡¶¨‡ßá‡¶° ‡¶≤‡¶ø‡¶Ç‡¶ï ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ
    if "facebook.com" in url or "fb.watch" in url:
        # ‡¶´‡ßá‡¶∏‡¶¨‡ßÅ‡¶ï‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø
        return f"https://www.facebook.com/plugins/video.php?href={url}&show_text=0&width=560"
    elif "instagram.com" in url:
        # ‡¶á‡¶®‡ßç‡¶∏‡¶ü‡¶æ‡¶ó‡ßç‡¶∞‡¶æ‡¶Æ‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø
        return f"https://www.instagram.com/p/{video_id}/embed"
    else:
        # ‡¶á‡¶â‡¶ü‡¶ø‡¶â‡¶¨ (‡¶°‡¶ø‡¶´‡¶≤‡ßç‡¶ü)
        return f"https://www.youtube-nocookie.com/embed/{video_id}?autoplay=0&rel=0"

def fetch_text_news():
    print("   üì∞ Robot: Reading Newspapers...")
    articles = []
    try:
        url = f"https://newsdata.io/api/1/latest?apikey={NEWS_API_KEY}&country=in&language=bn,hi,en&image=1&removeduplicate=1"
        res = requests.get(url, timeout=10).json()
        if res.get('status') == 'success':
            for item in res.get('results', [])[:6]:
                articles.append({
                    "id": item['article_id'],
                    "category": "breaking",
                    "title": item.get('title'),
                    "desc": item.get('description') or "Click to read full story...",
                    "thumb": item.get('image_url'),
                    "source": item.get('source_id'),
                    "video_url": "", # ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶®‡¶ø‡¶â‡¶ú‡ßá ‡¶≠‡¶ø‡¶°‡¶ø‡¶ì ‡¶®‡ßá‡¶á
                    "time": "Today",
                    "timestamp": time.time(),
                    "type": "image",
                    "platform": "news"
                })
    except: pass
    return articles

def fetch_social_videos(channels):
    video_news = []
    today_str, yesterday_str = get_smart_date()
    
    # yt_dlp ‡¶Ö‡¶™‡¶∂‡¶® (‡¶´‡¶æ‡¶∏‡ßç‡¶ü ‡¶∏‡ßç‡¶ï‡ßç‡¶Ø‡¶æ‡¶®‡¶ø‡¶Ç‡ßü‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø)
    ydl_opts = {
        'quiet': True, 
        'ignoreerrors': True, 
        'extract_flat': True, # ‡¶™‡ßÅ‡¶∞‡ßã ‡¶≠‡¶ø‡¶°‡¶ø‡¶ì ‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶° ‡¶®‡¶æ ‡¶ï‡¶∞‡ßá ‡¶∂‡ßÅ‡¶ß‡ßÅ ‡¶á‡¶®‡¶´‡ßã ‡¶®‡ßá‡¶¨‡ßá
        'playlistend': 5, 
        'socket_timeout': 15
    }

    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        for category, urls in channels.items():
            print(f"   üìÇ Robot: Scanning {category}...")
            for url in urls:
                if not url.startswith("http"): continue
                try:
                    info = ydl.extract_info(url, download=False)
                    
                    # ‡¶Ø‡¶¶‡¶ø ‡¶™‡ßç‡¶≤‡ßá‡¶≤‡¶ø‡¶∏‡ßç‡¶ü ‡¶¨‡¶æ ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶π‡ßü
                    entries = list(info['entries']) if 'entries' in info else [info]
                    
                    found = False
                    for video in entries:
                        if not video: continue
                        
                        # ‡¶≠‡¶ø‡¶°‡¶ø‡¶ì‡¶∞ ‡¶ß‡¶∞‡¶£ ‡¶ö‡ßá‡¶ï (Shorts/Landscape)
                        duration = video.get('duration', 0)
                        is_short = (duration > 0 and duration < 60) # ‡ß¨‡ß¶ ‡¶∏‡ßá‡¶ï‡ßá‡¶®‡ßç‡¶°‡ßá‡¶∞ ‡¶ï‡¶Æ ‡¶π‡¶≤‡ßá ‡¶∂‡¶∞‡ßç‡¶ü‡¶∏ ‡¶ß‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá
                        
                        video_id = video['id']
                        original_url = video.get('webpage_url', url) # ‡¶Ü‡¶∏‡¶≤ ‡¶≠‡¶ø‡¶°‡¶ø‡¶ì ‡¶≤‡¶ø‡¶Ç‡¶ï
                        
                        # ‡¶è‡¶Æ‡ßç‡¶¨‡ßá‡¶° ‡¶≤‡¶ø‡¶Ç‡¶ï ‡¶ú‡ßá‡¶®‡¶æ‡¶∞‡ßá‡¶ü
                        embed_link = get_embed_code(original_url, video_id)

                        # ‡¶•‡¶æ‡¶Æ‡ßç‡¶¨‡¶®‡ßá‡¶á‡¶≤ ‡¶π‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶°‡ßá‡¶≤‡¶ø‡¶Ç
                        thumb = video.get('thumbnail')
                        if not thumb:
                            thumb = f"https://i.ytimg.com/vi/{video_id}/hqdefault.jpg" # ‡¶á‡¶â‡¶ü‡¶ø‡¶â‡¶¨ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ï‡¶Ü‡¶™

                        video_news.append({
                            "id": video_id,
                            "category": category,
                            "title": video.get('title'),
                            "desc": video.get('title'),
                            "thumb": thumb,
                            "video_url": embed_link, # ‡¶∏‡ßç‡¶Æ‡¶æ‡¶∞‡ßç‡¶ü ‡¶è‡¶Æ‡ßç‡¶¨‡ßá‡¶° ‡¶≤‡¶ø‡¶Ç‡¶ï
                            "original_link": original_url, # ‡¶Ü‡¶∏‡¶≤ ‡¶≤‡¶ø‡¶Ç‡¶ï (‡¶´‡ßá‡¶∏‡¶¨‡ßÅ‡¶ï ‡¶¨‡¶æ ‡¶á‡¶â‡¶ü‡¶ø‡¶¨)
                            "source": info.get('uploader') or "Social Media",
                            "time": "Latest",
                            "timestamp": time.time(),
                            "type": "video",
                            "platform": "facebook" if "facebook" in original_url else "youtube"
                        })
                        found = True
                        if found: break # ‡¶™‡ßç‡¶∞‡¶§‡¶ø ‡¶≤‡¶ø‡¶Ç‡¶ï ‡¶•‡ßá‡¶ï‡ßá ‡ßß‡¶ü‡¶æ ‡¶≠‡¶ø‡¶°‡¶ø‡¶ì
                except: pass
    return video_news

def robot_loop():
    print("ü§ñ ROBOT SYSTEM: STARTED IN BACKGROUND")
    while True:
        try:
            config = load_config()
            channels = config.get("channels", {})
            location = config.get("location_override", "India")
            
            existing_db = load_db()
            existing_db = clean_old_news(existing_db)
            
            new_text = fetch_text_news()
            new_videos = fetch_social_videos(channels) # ‡¶®‡¶æ‡¶Æ ‡¶™‡¶∞‡¶ø‡¶¨‡¶∞‡ßç‡¶§‡¶® ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡ßá‡¶õ‡ßá
            fresh = new_text + new_videos
            
            # ‡¶°‡ßÅ‡¶™‡ßç‡¶≤‡¶ø‡¶ï‡ßá‡¶ü ‡¶ö‡ßá‡¶ï
            for item in fresh:
                if not any(ex['id'] == item['id'] for ex in existing_db):
                    existing_db.insert(0, item)
            
            with open(DB_FILE, 'w', encoding='utf-8') as f:
                json.dump({"news": existing_db, "updated_at": datetime.now().strftime("%I:%M %p"), "location": location}, f, indent=4, ensure_ascii=False)
            
            print(f"‚úÖ ROBOT: Cycle Complete. Active News: {len(existing_db)}")
            time.sleep(900) # ‡ßß‡ß´ ‡¶Æ‡¶ø‡¶®‡¶ø‡¶ü ‡¶ò‡ßÅ‡¶Æ
            
        except Exception as e:
            print(f"‚ùå ROBOT ERROR: {e}")
            time.sleep(60)

# ==========================================
# üé® PART 2: PROMO GENERATOR (Multi-Language Fix)
# ==========================================

def get_hashtags(title, lang):
    tags = ["#LPBSNews", "#Breaking"]
    title_lower = title.lower()
    
    # ‡¶≠‡¶æ‡¶á‡¶∞‡¶æ‡¶≤ ‡¶ï‡¶ø‡¶ì‡ßü‡¶æ‡¶∞‡ßç‡¶°
    keywords = {
        "bangladesh": "#Bangladesh", "india": "#India", "politics": "#Politics",
        "cricket": "#Cricket", "viral": "#Viral", "accident": "#News"
    }
    for key, tag in keywords.items():
        if key in title_lower: tags.append(tag)
    return " ".join(tags)

def create_viral_thumbnail(image_url, title, lang):
    if not PILLOW_AVAILABLE: return False
    
    try:
        # ‡ßß. ‡¶õ‡¶¨‡¶ø ‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶°
        response = requests.get(image_url)
        img = Image.open(io.BytesIO(response.content))
        img = img.convert("RGB")
        
        # ‡ß®. ‡¶∏‡¶æ‡¶á‡¶ú ‡¶†‡¶ø‡¶ï ‡¶ï‡¶∞‡¶æ (1280x720)
        # ‡¶Ø‡¶¶‡¶ø ‡¶õ‡¶¨‡¶ø ‡¶≤‡¶Æ‡ßç‡¶¨‡¶æ ‡¶π‡ßü (Shorts/Reels), ‡¶§‡¶æ‡¶π‡¶≤‡ßá ‡¶¨‡ßç‡¶≤‡¶æ‡¶∞ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ï‡¶ó‡ßç‡¶∞‡¶æ‡¶â‡¶®‡ßç‡¶° ‡¶¶‡ßá‡¶ì‡ßü‡¶æ ‡¶π‡¶¨‡ßá
        base_width = 1280
        base_height = 720
        canvas = Image.new("RGB", (base_width, base_height), (0,0,0))
        
        img_ratio = img.width / img.height
        target_ratio = base_width / base_height
        
        if img_ratio < target_ratio: 
            # ‡¶è‡¶ü‡¶æ ‡¶≠‡¶æ‡¶∞‡ßç‡¶ü‡¶ø‡¶ï‡¶æ‡¶≤ (‡¶≤‡¶Æ‡ßç‡¶¨‡¶æ) ‡¶≠‡¶ø‡¶°‡¶ø‡¶ì - ‡¶Æ‡¶æ‡¶ù‡¶ñ‡¶æ‡¶®‡ßá ‡¶¨‡¶∏‡¶¨‡ßá
            new_height = base_height
            new_width = int(new_height * img_ratio)
            img_resized = img.resize((new_width, new_height))
            
            # ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ï‡¶ó‡ßç‡¶∞‡¶æ‡¶â‡¶®‡ßç‡¶°‡ßá ‡¶¨‡ßç‡¶≤‡¶æ‡¶∞ ‡¶á‡¶´‡ßá‡¶ï‡ßç‡¶ü
            bg_blur = img.resize((base_width, base_height))
            bg_blur = bg_blur.filter(ImageFilter.GaussianBlur(radius=20))
            canvas.paste(bg_blur, (0,0))
            
            # ‡¶Ü‡¶∏‡¶≤ ‡¶õ‡¶¨‡¶ø ‡¶Æ‡¶æ‡¶ù‡¶ñ‡¶æ‡¶®‡ßá
            x_pos = (base_width - new_width) // 2
            canvas.paste(img_resized, (x_pos, 0))
            final_img = canvas
        else:
            # ‡¶è‡¶ü‡¶æ ‡¶®‡¶∞‡¶Æ‡¶æ‡¶≤ ‡¶≠‡¶ø‡¶°‡¶ø‡¶ì - ‡¶´‡ßÅ‡¶≤ ‡¶∏‡ßç‡¶ï‡ßç‡¶∞‡¶ø‡¶® ‡¶π‡¶¨‡ßá
            final_img = img.resize((base_width, base_height))

        # ‡ß©. ‡¶°‡ßç‡¶∞‡ßü‡¶ø‡¶Ç ‡¶ü‡ßÅ‡¶≤
        draw = ImageDraw.Draw(final_img)
        
        # ‡ß™. ‡¶´‡¶®‡ßç‡¶ü ‡¶∏‡¶ø‡¶≤‡ßá‡¶ï‡¶∂‡¶® (‡¶≤‡ßç‡¶Ø‡¶æ‡¶ô‡ßç‡¶ó‡ßÅ‡ßü‡ßá‡¶ú ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡ßü‡ßÄ)
        font_filename = FONTS.get(lang, FONTS['en']) # ‡¶°‡¶ø‡¶´‡¶≤‡ßç‡¶ü ‡¶á‡¶Ç‡¶≤‡¶ø‡¶∂
        
        try:
            if os.path.exists(font_filename):
                title_font = ImageFont.truetype(font_filename, 50)
                sub_font = ImageFont.truetype(font_filename, 35)
            else:
                # ‡¶Ø‡¶¶‡¶ø ‡¶´‡¶®‡ßç‡¶ü ‡¶®‡¶æ ‡¶™‡¶æ‡ßü, ‡¶°‡¶ø‡¶´‡¶≤‡ßç‡¶ü ‡¶≤‡ßã‡¶° ‡¶π‡¶¨‡ßá (‡¶¨‡¶ï‡ßç‡¶∏ ‡¶Ü‡¶∏‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá)
                print(f"‚ö†Ô∏è Font {font_filename} not found!")
                title_font = ImageFont.load_default()
                sub_font = ImageFont.load_default()
        except:
            title_font = ImageFont.load_default()
            sub_font = ImageFont.load_default()

        # ‡ß´. ‡¶ü‡ßç‡¶∞‡¶æ‡¶®‡ßç‡¶∏‡¶™‡¶æ‡¶∞‡ßá‡¶®‡ßç‡¶ü ‡¶ï‡¶æ‡¶≤‡ßã ‡¶∂‡ßá‡¶° (‡¶®‡¶ø‡¶ö‡ßá)
        overlay = Image.new('RGBA', final_img.size, (0,0,0,0))
        draw_overlay = ImageDraw.Draw(overlay)
        draw_overlay.rectangle([(0, 500), (1280, 720)], fill=(0, 0, 0, 180)) # 180 = ‡¶è‡¶ï‡¶ü‡ßÅ ‡¶ó‡¶æ‡ßù ‡¶ï‡¶æ‡¶≤‡ßã
        final_img = Image.alpha_composite(final_img.convert('RGBA'), overlay)
        final_img = final_img.convert('RGB')
        draw = ImageDraw.Draw(final_img)

        # ‡ß¨. ‡¶≤‡ßá‡¶ñ‡¶æ ‡¶¨‡¶∏‡¶æ‡¶®‡ßã
        short_title = title[:60] + "..." if len(title) > 60 else title
        
        # ‡¶ü‡¶æ‡¶á‡¶ü‡ßá‡¶≤ (‡¶π‡¶≤‡ßÅ‡¶¶)
        draw.text((30, 520), short_title, font=title_font, fill=(255, 235, 59)) 
        
        # ‡¶∏‡¶æ‡¶¨‡¶ü‡¶æ‡¶á‡¶ü‡ßá‡¶≤ (‡¶∏‡¶æ‡¶¶‡¶æ) - ‡¶≠‡¶æ‡¶∑‡¶æ ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡ßü‡ßÄ
        if lang == 'bn':
            subtitle = "‚ñ∂ ‡¶≠‡¶ø‡¶°‡¶ø‡¶ì‡¶∞ ‡¶≤‡¶ø‡¶Ç‡¶ï ‡¶™‡ßç‡¶∞‡¶•‡¶Æ ‡¶ï‡¶Æ‡ßá‡¶®‡ßç‡¶ü‡ßá üëá"
        elif lang == 'hi':
            subtitle = "‚ñ∂ ‡§µ‡•Ä‡§°‡§ø‡§Ø‡•ã ‡§ï‡§æ ‡§≤‡§ø‡§Ç‡§ï ‡§™‡§π‡§≤‡•á ‡§ï‡§Æ‡•á‡§Ç‡§ü ‡§Æ‡•á‡§Ç üëá"
        else:
            subtitle = "‚ñ∂ Video Link in First Comment üëá"
            
        draw.text((30, 600), subtitle, font=sub_font, fill=(255, 255, 255))

        # ‡ß≠. ‡¶∏‡ßá‡¶≠
        final_img.save(PROMO_IMAGE_FILE)
        return True
        
    except Exception as e:
        print(f"Thumbnail Error: {e}")
        return False

# ==========================================
# üåê PART 3: THE SERVER
# ==========================================

class MyRequestHandler(http.server.SimpleHTTPRequestHandler):
    def do_POST(self):
        if self.path == '/save_config':
            length = int(self.headers['Content-Length'])
            data = json.loads(self.rfile.read(length))
            with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=4, ensure_ascii=False)
            self.send_response(200); self.end_headers(); self.wfile.write(b"Saved")
        
        elif self.path == '/create_promo':
            length = int(self.headers['Content-Length'])
            data = json.loads(self.rfile.read(length))
            
            title = data.get('title', '')
            thumb_url = data.get('thumb', '')
            lang = data.get('lang', 'bn') # ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶°‡¶Æ‡¶ø‡¶® ‡¶™‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶•‡ßá‡¶ï‡ßá ‡¶Ü‡¶∏‡¶æ ‡¶≠‡¶æ‡¶∑‡¶æ
            
            hashtags = get_hashtags(title, lang)
            # ‡¶è‡¶ñ‡¶æ‡¶®‡ßá 'lang' ‡¶™‡¶æ‡¶†‡¶æ‡¶®‡ßã ‡¶π‡¶ö‡ßç‡¶õ‡ßá ‡¶Ø‡¶æ‡¶§‡ßá ‡¶∏‡¶†‡¶ø‡¶ï ‡¶´‡¶®‡ßç‡¶ü ‡¶≤‡ßã‡¶° ‡¶π‡ßü
            thumb_success = create_viral_thumbnail(thumb_url, title, lang)
            
            response_data = {
                "hashtags": hashtags,
                "status": "success" if thumb_success else "error",
                "image_url": f"/get_promo_image?t={int(time.time())}"
            }
            
            self.send_response(200)
            self.send_header('Content-type', 'application/json')
            self.end_headers()
            self.wfile.write(json.dumps(response_data).encode())

        else: self.send_error(404)

    def do_GET(self):
        if self.path == '/track_visit':
            self.update_stats()
            self.send_response(200); self.end_headers()
        elif self.path == '/get_stats':
            if os.path.exists("stats.json"):
                with open("stats.json", 'r') as f:
                    self.send_response(200); self.send_header('Content-type', 'application/json'); self.end_headers(); self.wfile.write(f.read().encode())
            else:
                self.send_response(200); self.wfile.write(b'{"total":0,"today":0}')
        
        elif self.path.startswith('/get_promo_image'):
            if os.path.exists(PROMO_IMAGE_FILE):
                self.send_response(200)
                self.send_header('Content-type', 'image/jpeg')
                self.end_headers()
                with open(PROMO_IMAGE_FILE, 'rb') as f:
                    self.wfile.write(f.read())
            else:
                self.send_error(404)
        else:
            super().do_GET()

    def update_stats(self):
        s_file = "stats.json"
        data = {"total": 0, "today": 0, "date": ""}
        if os.path.exists(s_file):
            try: with open(s_file, 'r') as f: data = json.load(f)
            except: pass
        today = datetime.now().strftime("%Y-%m-%d")
        if data["date"] != today: data["date"] = today; data["today"] = 0
        data["total"] += 1; data["today"] += 1
        with open(s_file, 'w') as f: json.dump(data, f)

if __name__ == "__main__":
    robot_thread = threading.Thread(target=robot_loop)
    robot_thread.daemon = True
    robot_thread.start()
    print(f"üî• SERVER STARTED ON PORT {PORT}")
    with socketserver.TCPServer(("0.0.0.0", PORT), MyRequestHandler) as httpd:
        httpd.serve_forever()
