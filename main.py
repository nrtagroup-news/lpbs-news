import http.server
import socketserver
import json
import os
import threading
import time
import requests
import yt_dlp
import random
import xml.etree.ElementTree as ET  # ‡¶ó‡ßÅ‡¶ó‡¶≤ ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶∏ ‡¶™‡ßú‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø
from datetime import datetime, timedelta
import io
import textwrap  # ‡¶•‡¶æ‡¶Æ‡ßç‡¶¨‡¶®‡ßá‡¶á‡¶≤‡ßá‡¶∞ ‡¶≤‡ßá‡¶ñ‡¶æ ‡¶∏‡ßÅ‡¶®‡ßç‡¶¶‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø

# --- ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ú‡¶ø‡¶ï ‡¶≤‡¶æ‡¶á‡¶¨‡ßç‡¶∞‡ßá‡¶∞‡¶ø (Pillow) ---
try:
    from PIL import Image, ImageDraw, ImageFont, ImageFilter
    PILLOW_AVAILABLE = True
except ImportError:
    PILLOW_AVAILABLE = False
    print("‚ö†Ô∏è WARNING: Pillow library not found! requirements.txt ‡¶è Pillow ‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§ ‡¶ï‡¶∞‡ßÅ‡¶®‡•§")

# --- ‡¶ï‡¶®‡¶´‡¶ø‡¶ó‡¶æ‡¶∞‡ßá‡¶∂‡¶® ---
PORT = int(os.environ.get("PORT", 8080))
CONFIG_FILE = "config.json"
DB_FILE = "news_db.json"
# ‡¶®‡¶ø‡¶â‡¶ú ‡¶è‡¶™‡¶ø‡¶Ü‡¶á ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ï‡¶Ü‡¶™‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶∞‡¶æ‡¶ñ‡¶æ ‡¶π‡¶≤‡ßã, ‡¶§‡¶¨‡ßá ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶ó‡ßÅ‡¶ó‡¶≤ ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶∏ ‡¶¨‡ßá‡¶∂‡¶ø ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶¨
NEWS_API_KEY = "pub_102fa773efa04ad2871534886e425eab" 
PROMO_IMAGE_FILE = "promo_image.jpg"

# üî• ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡ßß: ‡¶∞‡¶ø‡¶ü‡ßá‡¶®‡¶∂‡¶® ‡ß™‡ßÆ ‡¶ò‡¶£‡ßç‡¶ü‡¶æ (‡ß® ‡¶¶‡¶ø‡¶®) ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡ßá‡¶õ‡ßá
RETENTION_HOURS = 48 

FONTS = {
    'bn': 'bn.ttf',
    'hi': 'hn.ttf',
    'en': 'en.ttf',
    'tm': 'tm.ttf'
}

# ==========================================
# üß† PART 1: THE ROBOT BRAIN (INTELLIGENCE)
# ==========================================

def load_config():
    if not os.path.exists(CONFIG_FILE): return {}
    with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
        return json.load(f)

def load_db():
    if not os.path.exists(DB_FILE): return []
    try:
        with open(DB_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data.get("news", [])
    except: return []

# üî• ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡ß®: ‡¶≤‡¶ú‡¶ø‡¶ï ‡¶´‡¶ø‡¶ï‡ßç‡¶∏ - ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶ñ‡¶¨‡¶∞‡ßá‡¶∞ ‡¶¨‡ßü‡¶∏ ‡¶Ü‡¶≤‡¶æ‡¶¶‡¶æ‡¶≠‡¶æ‡¶¨‡ßá ‡¶ö‡ßá‡¶ï ‡¶π‡¶¨‡ßá
def clean_old_news(news_list):
    current_time = time.time()
    retention_seconds = RETENTION_HOURS * 3600
    cleaned_list = []
    for n in news_list:
        news_age = current_time - n.get('timestamp', 0)
        # ‡¶Ø‡¶¶‡¶ø ‡¶ñ‡¶¨‡¶∞‡ßá‡¶∞ ‡¶¨‡ßü‡¶∏ ‡ß™‡ßÆ ‡¶ò‡¶£‡ßç‡¶ü‡¶æ‡¶∞ ‡¶ï‡¶Æ ‡¶π‡ßü, ‡¶§‡¶¨‡ßá‡¶á ‡¶∏‡ßá‡¶ü‡¶æ ‡¶∞‡¶æ‡¶ñ‡¶æ ‡¶π‡¶¨‡ßá
        if news_age < retention_seconds:
            cleaned_list.append(n)
    return cleaned_list

def get_embed_code(url, video_id):
    if "facebook.com" in url or "fb.watch" in url:
        return f"https://www.facebook.com/plugins/video.php?href={url}&show_text=0&width=560"
    elif "instagram.com" in url:
        return f"https://www.instagram.com/p/{video_id}/embed"
    else:
        return f"https://www.youtube-nocookie.com/embed/{video_id}?autoplay=0&rel=0"

# üî• ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡ß©: Google Trending Topics (Real-time)
def fetch_google_trends():
    print("   üìà Robot: Checking Google Trends...")
    trends = []
    try:
        # ‡¶≠‡¶æ‡¶∞‡¶§‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶ó‡ßÅ‡¶ó‡¶≤ ‡¶°‡ßá‡¶á‡¶≤‡¶ø ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶∏ ‡¶Ü‡¶∞‡¶è‡¶∏‡¶è‡¶∏
        url = "https://trends.google.com/trends/trendingsearches/daily/rss?geo=IN"
        response = requests.get(url, timeout=10)
        if response.status_code == 200:
            root = ET.fromstring(response.content)
            for item in root.findall('.//item')[:5]: # ‡¶ü‡¶™ ‡ß´ ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°
                title = item.find('title').text
                # ‡¶°‡ßá‡¶∏‡¶ï‡ßç‡¶∞‡¶ø‡¶™‡¶∂‡¶® ‡¶ï‡ßç‡¶≤‡¶ø‡¶® ‡¶ï‡¶∞‡¶æ
                desc = f"Trending now in India: {title}. See full coverage on LPBS News."
                try:
                    news_item_title = item.find('ht:news_item_title', namespaces={'ht': 'https://trends.google.com/trends/trendingsearches/daily'}).text
                    desc = news_item_title
                except: pass
                
                image_url = "https://via.placeholder.com/600x400?text=Trending+News" # ‡¶´‡¶≤‡¶¨‡ßç‡¶Ø‡¶æ‡¶ï ‡¶á‡¶Æ‡ßá‡¶ú
                try:
                    image_url = item.find('ht:picture', namespaces={'ht': 'https://trends.google.com/trends/trendingsearches/daily'}).text
                except: pass

                trends.append({
                    "id": f"trend_{abs(hash(title))}",
                    "category": "Trending üî•",
                    "title": title,
                    "desc": desc,
                    "thumb": image_url,
                    "source": "Google Trends",
                    "video_url": "",
                    "time": "Hot Topic",
                    "timestamp": time.time(),
                    "type": "image",
                    "platform": "google"
                })
    except Exception as e:
        print(f"Trend Error: {e}")
    return trends

def fetch_social_videos(channels):
    video_news = []
    
    # üî• ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡ß™: playlistend ‡¶¨‡¶æ‡¶°‡¶º‡¶ø‡ßü‡ßá 15 ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡ßá‡¶õ‡ßá ‡¶Ø‡¶æ‡¶§‡ßá ‡¶∏‡¶ï‡¶æ‡¶≤‡ßá‡¶∞ ‡¶≠‡¶ø‡¶°‡¶ø‡¶ì ‡¶Æ‡¶ø‡¶∏ ‡¶®‡¶æ ‡¶π‡ßü
    ydl_opts = {
        'quiet': True, 
        'ignoreerrors': True, 
        'extract_flat': True,
        'playlistend': 15, # ‡¶Ü‡¶ó‡ßá ‡ß´ ‡¶õ‡¶ø‡¶≤, ‡¶è‡¶ñ‡¶® ‡ßß‡ß´ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶≤‡ßã
        'socket_timeout': 20
    }

    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        for category, urls in channels.items():
            print(f"   üìÇ Robot: Deep Scanning {category}...")
            for url in urls:
                if not url.startswith("http"): continue
                try:
                    info = ydl.extract_info(url, download=False)
                    entries = list(info['entries']) if 'entries' in info else [info]
                    
                    for video in entries:
                        if not video: continue
                        
                        video_id = video['id']
                        original_url = video.get('webpage_url', url)
                        embed_link = get_embed_code(original_url, video_id)
                        
                        thumb = video.get('thumbnail')
                        if not thumb: thumb = f"https://i.ytimg.com/vi/{video_id}/hqdefault.jpg"

                        video_news.append({
                            "id": video_id,
                            "category": category,
                            "title": video.get('title') or "Latest Update",
                            "desc": video.get('title') or "Click to watch",
                            "thumb": thumb,
                            "video_url": embed_link,
                            "original_link": original_url,
                            "source": info.get('uploader') or "Social Media",
                            "time": "Just Now",
                            "timestamp": time.time(), # ‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶® ‡¶∏‡¶Æ‡ßü ‡¶¶‡¶ø‡¶ö‡ßç‡¶õ‡¶ø, ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶è‡¶ü‡¶ø ‡¶°‡¶æ‡¶ü‡¶æ‡¶¨‡ßá‡¶∏‡ßá ‡¶•‡¶æ‡¶ï‡¶≤‡ßá ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶π‡¶¨‡ßá ‡¶®‡¶æ
                            "type": "video",
                            "platform": "facebook" if "facebook" in original_url else "youtube"
                        })
                except: pass
                # ‡¶á‡¶â‡¶ü‡¶ø‡¶â‡¶¨ ‡¶¨‡ßç‡¶≤‡¶ï ‡¶è‡ßú‡¶æ‡¶§‡ßá ‡¶õ‡ßã‡¶ü ‡¶¨‡¶ø‡¶∞‡¶§‡¶ø
                time.sleep(2) 
    return video_news

def robot_loop():
    print("ü§ñ ROBOT SYSTEM: INITIALIZED & POWERFUL")
    while True:
        try:
            config = load_config()
            channels = config.get("channels", {})
            location = config.get("location_override", "India")
            
            # ‡ßß. ‡¶Ü‡¶ó‡ßá‡¶∞ ‡¶°‡¶æ‡¶ü‡¶æ ‡¶≤‡ßã‡¶°
            existing_db = load_db()
            
            # ‡ß®. ‡¶™‡ßÅ‡¶∞‡¶®‡ßã ‡¶ñ‡¶¨‡¶∞ ‡¶°‡¶ø‡¶≤‡¶ø‡¶ü (‡ß™‡ßÆ ‡¶ò‡¶£‡ßç‡¶ü‡¶æ‡¶∞ ‡¶¨‡ßá‡¶∂‡¶ø ‡¶π‡¶≤‡ßá)
            existing_db = clean_old_news(existing_db)
            
            # ‡ß©. ‡¶®‡¶§‡ßÅ‡¶® ‡¶ñ‡¶¨‡¶∞ ‡¶Ü‡¶®‡¶æ (Google Trends + YouTube)
            new_trends = fetch_google_trends()
            new_videos = fetch_social_videos(channels)
            
            fresh_content = new_trends + new_videos
            
            # ‡ß™. ‡¶°‡ßÅ‡¶™‡ßç‡¶≤‡¶ø‡¶ï‡ßá‡¶ü ‡¶ö‡ßá‡¶ï ‡¶è‡¶¨‡¶Ç ‡¶Æ‡¶æ‡¶∞‡ßç‡¶ú ‡¶ï‡¶∞‡¶æ
            # ‡¶®‡¶§‡ßÅ‡¶® ‡¶ï‡¶®‡ßç‡¶ü‡ßá‡¶®‡ßç‡¶ü ‡¶Ø‡¶¶‡¶ø ‡¶á‡¶§‡¶ø‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶•‡¶æ‡¶ï‡ßá, ‡¶∏‡ßá‡¶ü‡¶æ ‡¶∏‡ßç‡¶ï‡¶ø‡¶™ ‡¶ï‡¶∞‡¶¨
            existing_ids = {item['id'] for item in existing_db}
            
            added_count = 0
            for item in fresh_content:
                if item['id'] not in existing_ids:
                    existing_db.insert(0, item) # ‡¶®‡¶§‡ßÅ‡¶® ‡¶ñ‡¶¨‡¶∞ ‡¶∏‡¶¨‡¶æ‡¶∞ ‡¶â‡¶™‡¶∞‡ßá
                    added_count += 1
            
            # ‡ß´. ‡¶°‡¶æ‡¶ü‡¶æ‡¶¨‡ßá‡¶∏ ‡¶∏‡ßá‡¶≠
            with open(DB_FILE, 'w', encoding='utf-8') as f:
                json.dump({
                    "news": existing_db, 
                    "updated_at": datetime.now().strftime("%I:%M %p"), 
                    "location": location,
                    "total_articles": len(existing_db)
                }, f, indent=4, ensure_ascii=False)
            
            print(f"‚úÖ ROBOT: Cycle Complete. New Items: {added_count}. Active News: {len(existing_db)}")
            
            # üî• ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡ß´: ‡¶´‡ßç‡¶∞‡¶ø‡¶ï‡ßã‡¶Ø‡¶º‡ßá‡¶®‡ßç‡¶∏‡¶ø
            # ‡¶∞‡ßã‡¶¨‡¶ü ‡¶è‡¶ñ‡¶® ‡¶™‡ßç‡¶∞‡¶§‡¶ø ‡ß´ ‡¶Æ‡¶ø‡¶®‡¶ø‡¶ü (‡ß©‡ß¶‡ß¶ ‡¶∏‡ßá‡¶ï‡ßá‡¶®‡ßç‡¶°) ‡¶™‡¶∞ ‡¶™‡¶∞ ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡¶¨‡ßá‡•§ 
            # ‡ßß‡ß´ ‡¶∏‡ßá‡¶ï‡ßá‡¶®‡ßç‡¶° ‡¶¶‡¶ø‡¶≤‡ßá ‡¶á‡¶â‡¶ü‡¶ø‡¶â‡¶¨ ‡¶Ü‡¶á‡¶™‡¶ø ‡¶¨‡ßç‡¶≤‡¶ï ‡¶ï‡¶∞‡ßá ‡¶¶‡ßá‡¶¨‡ßá, ‡¶§‡¶æ‡¶á ‡ß©‡ß¶‡ß¶ ‡¶∏‡ßá‡¶ï‡ßá‡¶®‡ßç‡¶° ‡¶®‡¶ø‡¶∞‡¶æ‡¶™‡¶¶ ‡¶è‡¶¨‡¶Ç ‡¶Ø‡¶•‡ßá‡¶∑‡ßç‡¶ü ‡¶´‡¶æ‡¶∏‡ßç‡¶ü‡•§
            time.sleep(300) 

        except Exception as e:
            print(f"‚ùå ROBOT ERROR: {e}")
            time.sleep(60)

# ==========================================
# üé® PART 2: SMART PROMO & THUMBNAIL
# ==========================================

# üî• ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡ß¨: ‡¶∏‡ßç‡¶Æ‡¶æ‡¶∞‡ßç‡¶ü ‡¶π‡ßç‡¶Ø‡¶æ‡¶∂‡¶ü‡ßç‡¶Ø‡¶æ‡¶ó ‡¶ú‡ßá‡¶®‡¶æ‡¶∞‡ßá‡¶ü‡¶∞
def get_hashtags(title, lang):
    title_lower = title.lower()
    tags = ["#LPBSNews", "#Latest"]
    
    # ‡¶ï‡¶ø‡¶ì‡ßü‡¶æ‡¶∞‡ßç‡¶° ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶™‡¶ø‡¶Ç
    keywords = {
        "bangladesh": "#Bangladesh", "india": "#India", "modi": "#PMModi",
        "mamata": "#MamataBanerjee", "cricket": "#Cricket", "football": "#Sports",
        "viral": "#ViralVideo", "accident": "#Breaking", "election": "#Election2026",
        "budget": "#Budget", "weather": "#WeatherUpdate", "job": "#Jobs"
    }
    
    for key, tag in keywords.items():
        if key in title_lower:
            tags.append(tag)
            
    # ‡¶Ø‡¶¶‡¶ø ‡¶ó‡ßÅ‡¶ó‡¶≤ ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶∏ ‡¶•‡ßá‡¶ï‡ßá ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶™‡¶æ‡¶ì‡ßü‡¶æ ‡¶Ø‡¶æ‡ßü (‡¶∏‡¶ø‡¶Æ‡ßÅ‡¶≤‡ßá‡¶∂‡¶®)
    tags.append("#TrendingNow")
    
    return " ".join(tags[:6]) # ‡¶∏‡¶∞‡ßç‡¶¨‡ßã‡¶ö‡ßç‡¶ö ‡ß¨‡¶ü‡¶ø ‡¶ü‡ßç‡¶Ø‡¶æ‡¶ó

def create_viral_thumbnail(image_url, title, lang):
    if not PILLOW_AVAILABLE: return False
    try:
        response = requests.get(image_url, timeout=5)
        img = Image.open(io.BytesIO(response.content)).convert("RGB")
        base_width, base_height = 1280, 720
        canvas = Image.new("RGB", (base_width, base_height), (0,0,0))
        
        # ‡¶á‡¶Æ‡ßá‡¶ú ‡¶∏‡ßç‡¶ï‡ßá‡¶≤‡¶ø‡¶Ç ‡¶è‡¶¨‡¶Ç ‡¶¨‡ßç‡¶≤‡¶æ‡¶∞ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ï‡¶ó‡ßç‡¶∞‡¶æ‡¶â‡¶®‡ßç‡¶°
        img_ratio = img.width / img.height
        target_ratio = base_width / base_height
        
        if img_ratio < target_ratio: 
            new_height = base_height
            new_width = int(new_height * img_ratio)
            img_resized = img.resize((new_width, new_height))
            bg_blur = img.resize((base_width, base_height)).filter(ImageFilter.GaussianBlur(radius=40))
            canvas.paste(bg_blur, (0,0))
            canvas.paste(img_resized, ((base_width - new_width) // 2, 0))
            final_img = canvas
        else:
            final_img = img.resize((base_width, base_height))

        # ‡¶ì‡¶≠‡¶æ‡¶∞‡¶≤‡ßá (‡¶®‡¶ø‡¶ö‡ßá‡¶∞ ‡¶¶‡¶ø‡¶ï‡ßá ‡¶ï‡¶æ‡¶≤‡ßã ‡¶∂‡ßá‡¶°)
        overlay = Image.new('RGBA', final_img.size, (0,0,0,0))
        draw_overlay = ImageDraw.Draw(overlay)
        # ‡¶∂‡ßá‡¶° ‡¶è‡¶ñ‡¶® ‡¶è‡¶ï‡¶ü‡ßÅ ‡¶¨‡ßú ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡ßá‡¶õ‡ßá ‡¶Ø‡¶æ‡¶§‡ßá ‡ß® ‡¶≤‡¶æ‡¶á‡¶®‡ßá‡¶∞ ‡¶≤‡ßá‡¶ñ‡¶æ ‡¶ß‡¶∞‡ßá
        draw_overlay.rectangle([(0, 450), (1280, 720)], fill=(0, 0, 0, 200)) 
        final_img = Image.alpha_composite(final_img.convert('RGBA'), overlay).convert('RGB')
        draw = ImageDraw.Draw(final_img)

        # ‡¶´‡¶®‡ßç‡¶ü ‡¶≤‡ßã‡¶°‡¶ø‡¶Ç
        font_filename = FONTS.get(lang, 'en.ttf')
        try:
            # ‡¶´‡¶®‡ßç‡¶ü ‡¶∏‡¶æ‡¶á‡¶ú ‡¶è‡¶ï‡¶ü‡ßÅ ‡¶õ‡ßã‡¶ü ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡ßá‡¶õ‡ßá ‡¶Ø‡¶æ‡¶§‡ßá ‡¶≤‡¶æ‡¶á‡¶® ‡¶¨‡ßç‡¶∞‡ßá‡¶ï ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡ßü
            if os.path.exists(font_filename):
                title_font = ImageFont.truetype(font_filename, 60)
                sub_font = ImageFont.truetype(font_filename, 40)
                logo_font = ImageFont.truetype(font_filename, 35)
            else:
                title_font = ImageFont.load_default()
                sub_font = ImageFont.load_default()
                logo_font = ImageFont.load_default()
        except:
            title_font = ImageFont.load_default(); sub_font = ImageFont.load_default(); logo_font = ImageFont.load_default()

        # ‡¶≤‡ßã‡¶ó‡ßã (‡¶â‡¶™‡¶∞‡ßá ‡¶¨‡¶æ‡¶Æ‡ßá)
        draw.rectangle([(20, 20), (240, 70)], fill="#D32F2F")
        draw.text((35, 25), "LPBS NEWS", font=logo_font, fill="white")

        # üî• ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡ß≠: ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶∞‚Äç‡ßç‡¶Ø‡¶æ‡¶™‡¶ø‡¶Ç (Text Wrapping) - ‡¶Ø‡¶æ‡¶§‡ßá ‡¶≤‡ßá‡¶ñ‡¶æ ‡¶ï‡ßá‡¶ü‡ßá ‡¶®‡¶æ ‡¶Ø‡¶æ‡ßü
        # ‡¶è‡¶¨‡¶Ç ‡¶Æ‡ßÅ‡¶ñ‡ßá‡¶∞ ‡¶â‡¶™‡¶∞ ‡¶®‡¶æ ‡¶™‡ßú‡ßá (‡¶®‡¶ø‡¶ö‡ßá ‡¶∏‡ßá‡¶ü ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡ßá‡¶õ‡ßá)
        margin = 40
        para = textwrap.wrap(title, width=45) # ‡¶™‡ßç‡¶∞‡¶§‡¶ø ‡¶≤‡¶æ‡¶á‡¶®‡ßá ‡ß™‡ß´ ‡¶ï‡ßç‡¶Ø‡¶æ‡¶∞‡ßá‡¶ï‡ßç‡¶ü‡¶æ‡¶∞
        
        current_h = 470
        for line in para[:2]: # ‡¶∏‡¶∞‡ßç‡¶¨‡ßã‡¶ö‡ßç‡¶ö ‡ß® ‡¶≤‡¶æ‡¶á‡¶® ‡¶™‡ßç‡¶∞‡¶ø‡¶®‡ßç‡¶ü ‡¶ï‡¶∞‡¶¨‡ßá
            draw.text((margin, current_h), line, font=title_font, fill=(255, 255, 0), stroke_width=3, stroke_fill="black")
            current_h += 75

        # ‡¶∏‡¶æ‡¶¨‡¶ü‡¶æ‡¶á‡¶ü‡ßá‡¶≤ (‡¶ï‡¶≤ ‡¶ü‡ßÅ ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶ï‡¶∂‡¶®)
        if lang == 'bn': subtitle = "‚ñ∂ ‡¶≠‡¶ø‡¶°‡¶ø‡¶ì ‡¶¶‡ßá‡¶ñ‡¶§‡ßá ‡¶ï‡ßç‡¶≤‡¶ø‡¶ï ‡¶ï‡¶∞‡ßÅ‡¶®"
        elif lang == 'hi': subtitle = "‚ñ∂ ‡§µ‡•Ä‡§°‡§ø‡§Ø‡•ã ‡§¶‡•á‡§ñ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ï‡•ç‡§≤‡§ø‡§ï ‡§ï‡§∞‡•á‡§Ç"
        else: subtitle = "‚ñ∂ Watch Full Video"
        
        draw.text((margin, 630), subtitle, font=sub_font, fill="white", stroke_width=2, stroke_fill="black")

        final_img.save(PROMO_IMAGE_FILE)
        return True
    except Exception as e:
        print(f"Thumbnail Error: {e}")
        return False

# ==========================================
# üåê PART 3: SERVER HANDLER
# ==========================================

class MyRequestHandler(http.server.SimpleHTTPRequestHandler):
    def do_POST(self):
        if self.path == '/save_config':
            length = int(self.headers['Content-Length'])
            data = json.loads(self.rfile.read(length))
            with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=4, ensure_ascii=False)
            self.send_response(200); self.end_headers(); self.wfile.write(b"Saved")
        
        elif self.path == '/create_promo':
            length = int(self.headers['Content-Length'])
            data = json.loads(self.rfile.read(length))
            title = data.get('title', '')
            thumb_url = data.get('thumb', '')
            lang = data.get('lang', 'bn')
            
            hashtags = get_hashtags(title, lang)
            thumb_success = create_viral_thumbnail(thumb_url, title, lang)
            
            self.send_response(200)
            self.send_header('Content-type', 'application/json')
            self.end_headers()
            self.wfile.write(json.dumps({
                "hashtags": hashtags, 
                "status": "success" if thumb_success else "error", 
                "image_url": f"/get_promo_image?t={int(time.time())}"
            }).encode())
        else:
            self.send_error(404)

    def do_GET(self):
        if self.path == '/track_visit':
            self.update_stats()
            self.send_response(200); self.end_headers()
        elif self.path == '/get_stats':
            if os.path.exists("stats.json"):
                with open("stats.json", 'r') as f:
                    self.send_response(200); self.send_header('Content-type', 'application/json'); self.end_headers()
                    self.wfile.write(f.read().encode())
            else:
                self.send_response(200); self.wfile.write(b'{"total":0,"today":0}')
        elif self.path.startswith('/get_promo_image'):
            if os.path.exists(PROMO_IMAGE_FILE):
                self.send_response(200); self.send_header('Content-type', 'image/jpeg'); self.end_headers()
                with open(PROMO_IMAGE_FILE, 'rb') as f:
                    self.wfile.write(f.read())
            else:
                self.send_error(404)
        else:
            super().do_GET()

    def update_stats(self):
        s_file = "stats.json"
        data = {"total": 0, "today": 0, "date": ""}
        if os.path.exists(s_file):
            try:
                with open(s_file, 'r') as f:
                    data = json.load(f)
            except: pass
        
        today = datetime.now().strftime("%Y-%m-%d")
        if data["date"] != today: data["date"] = today; data["today"] = 0
        data["total"] += 1; data["today"] += 1
        with open(s_file, 'w') as f: json.dump(data, f)

if __name__ == "__main__":
    # ‡¶∞‡ßã‡¶¨‡¶ü ‡¶•‡ßç‡¶∞‡ßá‡¶° ‡¶∏‡ßç‡¶ü‡¶æ‡¶∞‡ßç‡¶ü
    robot_thread = threading.Thread(target=robot_loop)
    robot_thread.daemon = True
    robot_thread.start()
    
    print(f"üî• LPBS SUPER-ROBOT STARTED ON PORT {PORT}")
    print(f"   üëâ Retention: {RETENTION_HOURS} Hours | Deep Search: ON | Google Trends: ON")
    
    with socketserver.TCPServer(("0.0.0.0", PORT), MyRequestHandler) as httpd:
        httpd.serve_forever()
